\documentclass[12pt]{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[a4paper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{jlcode}
\usepackage[toc,page]{appendix}

%macros
\newcommand{\defeq}[2]{\stackrel{\mathclap{\normalfont\mbox{#1}}}{#2}}
\def\D{\displaystyle}
\def\att{                    % mark at the margin
        \marginpar[ \hspace*{\fill} \raisebox{-0.2em}{\rule{2mm}{1.2em}} ]
        {\raisebox{-0.2em}{\rule{2mm}{1.2em}} }
        }
\def\at#1{[*** \att #1 ***]}  % text needing attention
\def\spc{\hspace*{0.5cm}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}


\title{2022W 510009-1 VGSCO: \\ Optimization Foundations of Reinforcement Learning}
\author{Beryl Ramadhian Aribowo}

\begin{document}
\maketitle
\section*{Assignment 1}
\subsection*{1. Basic concepts}
Let $\pi_0$ be a policy which always switches states.
For example, if we start from $A$, then the sequence of the visited states will be $A,B,A,B,A....$
\subsubsection*{a)}
The value function is defined by
\begin{equation}
    \begin{split}
        V^{\pi_0}(s) &= \mathbb{E}\Big[\sum^\infty_{t=0}\gamma^tr(s_t,a_t)|s_0=s, \pi_0\Big].
    \end{split}
    \label{eq:valuefun}
\end{equation}
For state $A$:
\begin{equation*}
    V^{\pi_0}(A) = 0 + \gamma + 0 + \gamma^3 + ... = \frac{\gamma}{1-\gamma^2}.
\end{equation*}
For state $B$:
\begin{equation*}
    V^{\pi_0}(B) = 1 + 0 + \gamma^2 + 0 + ... = \frac{1}{1-\gamma^2}.
\end{equation*}
Therefore
\begin{equation}
    \mathbf{V}^{\pi_0} := \Big[\frac{\gamma}{1-\gamma^2}, \frac{1}{1-\gamma^2}\Big]^\top.
    \label{eq:vpi0}
\end{equation}
\subsubsection*{b)}
By observation, the optimal policy $\pi^*$ is
\begin{equation*}
    \begin{split}
        &s_t=A \rightarrow \pi^*(s_t) = \text{stay}, \\
        &s_t=B \rightarrow \pi^*(s_t) = \text{switch} ,
    \end{split}    
\end{equation*}
Using (\ref{eq:valuefun}), the value functions by following $\pi^*$ are
\begin{equation*}
    \begin{split}
        V^{\pi^*}(A) &= 1 + \gamma + \gamma^2 + ... = \frac{1}{1-\gamma} ,\\
        V^{\pi^*}(B) &= 1 + \gamma + \gamma^2 + ... = \frac{1}{1-\gamma}.
    \end{split}
\end{equation*}
\subsubsection*{c)}
Bellman optimality operator is
\begin{equation*}
    \mathcal{T}V(s) := \max_{a \in \mathcal{A}} \Big[r(s,a) + \gamma\sum_{s'\in S}P(s'|s,a)V(s')\Big].
\end{equation*}
Given $\mathbf{V}^{\pi_0}$ from (\refeq{eq:vpi0}), then
\begin{equation}
    \begin{split}
        \mathcal{T}V^{\pi_0}(A) &= \max_{a \in \mathcal{A}} \Big[r(A,a) + \gamma P(A|A,a)V^{\pi_0}(A) + \gamma P(B|A,a)V^{\pi_0}(B) \Big] \\
        &= \max \Big[r(A,\text{stay}) + \gamma P(A|A,\text{stay})V^{\pi_0}(A) + \gamma P(B|A,\text{stay})V^{\pi_0}(B), \\ &r(A,\text{switch}) + \gamma P(A|A,\text{switch})V^{\pi_0}(A) + \gamma P(B|A,\text{switch})V^{\pi_0}(B)\Big] \\
        &= \max \Big[1 + \frac{\gamma^2}{1-\gamma^2} + 0, 0 + 0 + \frac{\gamma}{1-\gamma^2}\Big] \\
        &= \max \Big[\frac{1}{1-\gamma^2}, \frac{\gamma}{1-\gamma^2}\Big] \\
        &= \frac{1}{1-\gamma^2}, \\
        \mathcal{T}V^{\pi_0}(B) &= \max_{a \in \mathcal{A}} \Big[r(B,a) + \gamma P(A|B,a)V^{\pi_0}(A) + \gamma P(B|B,a)V^{\pi_0}(B) \Big] \\
        &= \dots \\
        &= \max \Big[\frac{\gamma}{1-\gamma^2}, \frac{1}{1-\gamma^2}\Big] \\
        &= \frac{1}{1-\gamma^2}.
    \end{split}
    \label{eq:bellmanop}
\end{equation}
\subsubsection*{d)}
The greedy policy of a state $s$ is defined by
\begin{equation*}
    \pi_{t+1}(s) = \argmax_{a\in\mathcal{A}}\Big[r(s,a) + \gamma \sum_{s'\in S} P(s'|s,a)V^{\pi_t}(s') \Big].
\end{equation*}
By the results of (\refeq{eq:bellmanop}) we can infer that the greedy policy given $\mathbf{V}^{\pi_0}$ is
\begin{equation*}
    \begin{split}
        \pi_1(A) &= \text{stay}, \\
        \pi_1(B) &= \text{switch}.
    \end{split}
\end{equation*}
\subsubsection*{e)}
Value iteration for each time step $t$ is defined by
\begin{equation*}
    \mathbf{V}_{t+1} = \mathcal{T}\mathbf{V}_t.
\end{equation*}
From (\refeq{eq:bellmanop}) we computed value iteration once given $\mathbf{V}^{\pi_0}$, and we can see that
\begin{equation*}
    \mathcal{T}V^{\pi_0}(A) = \mathcal{T}V^{\pi_0}(B) = V_1(A) = V_1(B) = \frac{1}{1-\gamma^2},
\end{equation*}
therefore we only need to do value iteration either for state $A$ or $B$, for 4 more times,
\begin{equation}
    \begin{split}
        V_2(A)&= \max \Big[1 + \gamma\Big(\frac{1}{1-\gamma^2}\Big), \frac{\gamma}{1-\gamma^2}\Big] = \frac{1+\gamma-\gamma^2}{1-\gamma^2}, \\
        V_3(A)&=\max \Big[1 + \gamma\Big(\frac{1+\gamma-\gamma^2}{1-\gamma^2}\Big), \frac{1+\gamma-\gamma^2}{1-\gamma^2}\Big] = \frac{1+\gamma-\gamma^3}{1-\gamma^2}, \\
        \dots \\
        V_5(A)&=V_5(B) = \frac{1+\gamma-\gamma^5}{1-\gamma^2}.
    \end{split}
    \label{eq:valiter}
\end{equation}
\subsubsection*{f)}
In policy iteration, for each time step $t$, we either compute value iteration until convergence or
\begin{equation}
    \mathbf{V}^{\pi_t} = (\mathbf{I}-\gamma\mathbf{P}^{\pi_t})^{-1}\mathbf{R}^{\pi_t},
\end{equation}
then update the policy $\pi_{t+1}$ by greedy policy.
Let us proceed by using the value iteration. From (\refeq{eq:valiter}), given an infinite number of iterations, the value iteration gives
\begin{equation}
    \begin{split}
        V^*(A) &= V^*(B) =\frac{1 + \gamma -\gamma^\infty}{1-\gamma^2} \\
        &= \frac{1+\gamma}{1-\gamma^2} - \underbrace{\frac{\gamma^\infty}{1-\gamma^2}}_{\approx 0} \\
        &= \frac{1+\gamma}{1-\gamma^2},
    \end{split}
    \label{eq:valiterinfinite}
\end{equation}
then we do a greedy policy, similarly to (\refeq{eq:bellmanop}) and by using (\refeq{eq:valiterinfinite}) we obtain
\begin{equation}
    \begin{split}
        \pi_{t+1}(A) &= \argmax \Big[1+\gamma( \frac{1+\gamma}{1-\gamma^2}),  \frac{1+\gamma}{1-\gamma^2}\Big] \\
        &= \text{stay} \\
        \pi_{t+1}(B) &= \argmax \Big[\frac{1+\gamma}{1-\gamma^2}, 1+\gamma( \frac{1+\gamma}{1-\gamma^2})\Big] \\
        &= \text{switch}
    \end{split}
    \label{eq:policyiter}
\end{equation}
for $t=2,..,5$, the value functions are already converged, with the same values as in (\refeq{eq:valiterinfinite}) which also give the same policy as in (\refeq{eq:policyiter}).

\subsection*{2. Bounding suboptimality via Bellman error}
\subsubsection*{a)}
Bellman optimality operator for state-action value function can be defined in a similar way to the Bellman optimality operator for state value function. Bellman optimality operator for state-action value function is
\begin{equation}
    \mathcal{T}_QQ(s,a) = r(s,a) + \gamma\Big[\sum_{s'\in S}P(s'|s,a) \Big(\max_{a' \in \mathcal{A}}Q(s',a')\Big)\Big].
    \label{eq:bellmanoptimalityop}
\end{equation}
From the lecture note, we have the relation between $Q$ and $V$ following policy $\pi$ as
\begin{equation}
    V^\pi(s) = \sum_{a \in \mathcal{A}}\pi(a|s)Q^\pi(s,a), 
    \label{eq:qvrelation}
\end{equation}
and from the lecture we have a bellman expectation operator for state value function
\begin{equation}
    V^\pi(s) = \mathcal{T}^\pi V^\pi(s) := \sum_{a \in \mathcal{A}} \pi(a|s) \Big[r(s,a) + \gamma \sum_{s'\in S}P(s'|s,a)V^\pi(s') \Big].
    \label{eq:vpi}
\end{equation}
Given $Q^\pi$:
\begin{equation}
    Q^\pi(s,a) = r(s,a) + \gamma\sum_{s'\in S}P(s'|s,a)V^\pi(s'),
    \label{eq:qfun}
\end{equation}
and by using (\refeq{eq:qvrelation}) we can define the bellman expectation operator for state-action value function similarly to (\refeq{eq:vpi}),
\begin{equation}
    \mathcal{T}^\pi_QQ(s,a) = r(s,a) + \gamma \sum_{s'\in S}P(s'|s,a)\sum_{a' \in \mathcal{A}}\pi(a'|s')Q(s',a').
    \label{eq:qbellmanexpect}
\end{equation}
\subsubsection*{b)}
Given any state action value functions
\begin{equation}
    \Big|\max_{a \in \mathcal{A}}\mathbf{Q}(s,a) - \max_{a \in \mathcal{A}}\mathbf{Q'}(s,a)\Big| \leq \|\mathbf{Q}-\mathbf{Q'}\|_{\infty} \quad \forall s\in S.
    \label{eq:qboundinfnorm}
\end{equation}
This can be shown by expanding the RHS:
\begin{equation*}
    \begin{split}
        \|\mathbf{Q}-\mathbf{Q'}\|_{\infty} &= \max_{a \in \mathcal{A}}\sum_{s\in S} \Big|\mathbf{Q}_{as} - \mathbf{Q}'_{as}\Big| \\
        &\geq \sum_{s\in S} \left|\max_{a \in \mathcal{A}} \mathbf{Q}_{as} - \max_{a \in \mathcal{A}} \mathbf{Q}'_{as}\right|,
    \end{split}
\end{equation*}
hence
\begin{equation*}
    \begin{split}
        \Big|\max_{a \in \mathcal{A}}\mathbf{Q}(s,a) - \max_{a \in \mathcal{A}}\mathbf{Q'}(s,a)\Big| \leq \sum_{s\in S} \left|\max_{a \in \mathcal{A}} \mathbf{Q}_{as} - \max_{a \in \mathcal{A}} \mathbf{Q}'_{as}\right| \leq \|\mathbf{Q}-\mathbf{Q'}\|_{\infty} ~~ \forall s \in S.
    \end{split}
\end{equation*}
$\mathcal{T}_Q$ is $\gamma$ contraction under $l_\infty$-norm, formally
\begin{equation}
    \|\mathcal{T}_Q\mathbf{Q}-\mathcal{T}_Q\mathbf{Q}'\|_\infty \leq \gamma \|\mathbf{Q}-\mathbf{Q'}\|_\infty,
\end{equation}
it can be shown by taking the absolute difference of the operator applied on each $\mathbf{Q}'$ and $\mathbf{Q}$ by using (\refeq{eq:bellmanoptimalityop}),
\begin{equation*}
    \begin{split}
        \Big|\mathcal{T}_Q\mathbf{Q}(s,a)-\mathcal{T}_Q\mathbf{Q}(s,a)'\Big| &= \Big|\gamma \sum_{s'\in S}P(s'|s,a)\Big(\max_{a' \in \mathcal{A}}\mathbf{Q}(s',a')-\max_{a' \in \mathcal{A}}\mathbf{Q}'(s',a')\Big)\Big| \\
        &= \gamma \sum_{s'\in S}P(s'|s,a)\Big|\max_{a' \in \mathcal{A}}\mathbf{Q}(s',a')-\max_{a' \in \mathcal{A}}\mathbf{Q}'(s',a')\Big| \\
        &\leq \gamma \sum_{s'\in S} \Big|\max_{a' \in \mathcal{A}}\mathbf{Q}(s',a')-\max_{a' \in \mathcal{A}}\mathbf{Q}'(s',a')\Big|, \\
    \end{split}
\end{equation*}
then by (\refeq{eq:qboundinfnorm})
\begin{equation*}
    \begin{split}
        \gamma \sum_{s'\in S}\Big|\max_{a' \in \mathcal{A}}\mathbf{Q}(s',a')-\max_{a' \in \mathcal{A}}\mathbf{Q}'(s',a')\Big| \leq \gamma \|\mathbf{Q} - \mathbf{Q'}\|_\infty,
    \end{split}
\end{equation*}
therefore
\begin{equation*}
    \Big|\mathcal{T}_Q\mathbf{Q}(s,a)-\mathcal{T}_Q\mathbf{Q}(s,a)'\Big| \leq \gamma \|\mathbf{Q} - \mathbf{Q'}\|_\infty.
\end{equation*}
$\mathcal{T}^\pi_Q$ is also a $\gamma$ contraction under $l_\infty$-norm, by a similar manner to $\mathcal{T}_Q$:
\begin{equation}
    \begin{split}
        \Big|\mathcal{T}^\pi_Q\mathbf{Q}(s,a)-\mathcal{T}^\pi_Q\mathbf{Q}(s,a)'\Big| &= \Big|\gamma \sum_{s'\in S}P(s'|s,a)\sum_{a'\in \mathcal{A}}\pi(a'|s')\Big(\mathbf{Q}(s',a')-\mathbf{Q}'(s',a')\Big)\Big| \\
        &= \gamma \sum_{s'\in S}P(s'|s,a)\sum_{a'\in \mathcal{A}}\pi(a'|s')\Big|\mathbf{Q}(s',a')-\mathbf{Q}'(s',a')\Big| \\
        &\leq \gamma \|\mathbf{Q} - \mathbf{Q'}\|_\infty.
    \end{split}
\end{equation}
\subsubsection*{c)}
For any state-action value function $\mathbf{Q}$:
\begin{equation}
    \|\mathbf{Q} - \mathbf{Q}^*\|_\infty \leq \frac{\|\mathbf{Q} - \mathcal{T}_Q\mathbf{Q}\|_\infty}{1-\gamma}.
    \label{eq:2cproblem}
\end{equation}
We can use the fixed point definition:
\begin{equation}
    \mathbf{Q}^* = \mathcal{T}_Q\mathbf{Q}^*,
\end{equation}
and the result of \textbf{2.b)} to prove (\refeq{eq:2cproblem}): 
\begin{equation*}
    \begin{split}
        \|\mathbf{Q} - \mathbf{Q}^*\|_\infty &= \|\mathbf{Q} - \mathcal{T}_Q\mathbf{Q} + \mathcal{T}_Q\mathbf{Q} -\mathcal{T}_Q\mathbf{Q}^*\|_\infty \\
        &\leq \|\mathbf{Q} - \mathcal{T}_Q\mathbf{Q}\|_\infty + \|\mathcal{T}_Q\mathbf{Q} -\mathcal{T}_Q\mathbf{Q}^*\|_\infty \\
        &\leq \|\mathbf{Q} - \mathcal{T}_Q\mathbf{Q}\|_\infty + \gamma \|\mathbf{Q} - \mathbf{Q}^*\|_\infty.
    \end{split}
\end{equation*}

\subsubsection*{d)}
Let $\pi$ be a greedy policy with respect to $Q$,
\begin{equation}
    \pi = \argmax_a Q(\cdot, a),
\end{equation}
value function $V^\pi$ for this policy satisfies
\begin{equation}
    \|\mathbf{V}^\pi - \mathbf{V}^*\|_\infty \leq \frac{2\|\mathbf{Q}-\mathcal{T}_Q\mathbf{Q}\|_\infty}{1-\gamma}.
    \label{eq:2dquest}
\end{equation}
We can rewrite the definition of $V^\pi$ in (\refeq{eq:vpi}) in terms of $Q^\pi$
\begin{equation}
    V^\pi(s) = \sum_{a \in \mathcal{A}} \pi(a|s) Q^\pi(s,a).
    \label{eq:vpiinq}
\end{equation}
Next, $V^*$ in terms of $Q^*$ is
\begin{equation}
    V^*(s) = \max_{a\in \mathcal{A}}Q^*(s,a).
    \label{eq:vstar}
\end{equation}
We expand LHS of (\refeq{eq:2dquest}) by using (\refeq{eq:vpiinq}) and (\refeq{eq:vstar})
\begin{equation*}
    \begin{split}
        \|\mathbf{V}^\pi - \mathbf{V}^*\|_\infty &= \|\sum_{a \in \mathcal{A}} \pi(a|s) \mathbf{Q}^\pi(s,a) - \max_{a\in A}\mathbf{Q}^*(s,a)\|_\infty \\
        &\leq \|\max_{a\in \mathcal{A}}\mathbf{Q}(s,a) - \max_{a\in A}\mathbf{Q}^*(s,a)\|_\infty \\
        &\leq 2\|\max_{a\in \mathcal{A}}\mathbf{Q}(s,a) - \max_{a\in A}\mathbf{Q}^*(s,a)\|_\infty,
    \end{split}
\end{equation*}
then by using (\refeq{eq:qboundinfnorm}) and (\refeq{eq:2cproblem})
\begin{equation*}
    \begin{split}
        2\|\max_{a\in \mathcal{A}}\mathbf{Q}(s,a) - \max_{a\in A}\mathbf{Q}^*(s,a)\|_\infty &\leq 2 \|\mathbf{Q}-\mathbf{Q}^*\|_\infty \\
        &\leq \frac{2\|\mathbf{Q} - \mathcal{T}_Q\mathbf{Q}\|_\infty}{1-\gamma}.
    \end{split}
\end{equation*}
\subsection*{3. Convergence of inexact policy iteration}
An inexact policy iteration in which we compute $\mathbf{V}_t$ such that
\begin{equation*}
    \max_{s \in S}\left|\mathbf{V}_t(s) - \mathbf{V}^{\pi_t}(s)\right| \leq \epsilon
\end{equation*}
for some $\epsilon > 0$, with a greedy policy for $\pi_{t+1}$ with respect to $\mathbf{V}_t$, satisfies
\begin{equation}
    \lim_{t \rightarrow \infty} \sup \max_{s \in S}\left| \mathbf{V}^{\pi_t}(s) - \mathbf{V}^*(s)\right| \leq \frac{2\gamma\epsilon}{(1-\gamma)^2}.
\end{equation}
To show this, we can use the fact that both Bellman operators are $\gamma$-contractions, as shown in the results of \textbf{2.b)}, the convergence of policy iteration
\begin{equation*}
    \|\mathbf{V}^{\pi_t} - \mathbf{V}^*\|_\infty \leq \gamma^t\|\mathbf{V}^{\pi_0} - \mathbf{V}^*\|_\infty,
\end{equation*}
and
\begin{equation*}
    \mathbf{V}^* \geq \mathbf{V}^{\pi_t} \geq \mathcal{T}\mathbf{V}^{\pi_{t-1}},
\end{equation*}
where an additional error term could be induced each time the Bellman operators are applied.

\section*{Assignment 2}
\subsection*{1. Monte Carlo and TD learning}
For this exercise, all of the algorithms are implemented in Julia programming language, the complete code is attached in the Appendix (also available in my github repository: \url{https://github.com/berylgithub/ORLex/blob/main/rl.jl}).

Consider an MDP $\mathcal{M} = (\mathcal{S}, \mathcal{A}, \mathcal{P}, r,\mu, \gamma)$ with
\begin{equation*}
    \begin{split}
        &\mathcal{S} = \{A,B,T\}, ~~ \mathcal{A} = \{\text{switch},\text{stay}\}, \\
        &r(A, \text{switch})=0, ~~ r(A, \text{stay})=1, ~~ r(B, \text{switch})=1, ~~ r(B, \text{stay})=2, \\
        &P(B|A, \text{switch})=1, ~~ P(A|A, \text{stay})=1, ~~P(A|B, \text{switch})=1, ~~P(T|B, \text{stay})=1.
    \end{split}
\end{equation*}
\subsubsection*{a)}
First-visit MC (FVMC) averages the returns following first visits to $s$, while every-fisit MC (EVMC) averages the returns following all visits to $s$. The return at time step $t$ is defined by
\begin{equation}
    \begin{split}
        G_t &= r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + ... \\
            &= r_{t+1} +\gamma(r_{t+2} + \gamma^2 r_{t+3} + ...) \\
            &= r_{t+1} +\gamma(G_{t+1}),
    \end{split}
\end{equation}
from this formula, we can compute the returns for both MC algorithms backward,i.e., $t=T-1,...,0$. 
The codes for both FVMC and EVMC are implemented by computing the returns backward. 
The function which computes the estimation of $V^{\pi}(s)$ using the FVMC is as the following:
\begin{jllisting}
    function fvmc(states, S, R, γ)
        n = zeros(Int, length(states)) # n_ts[i] := counter of visiting state i
        Vs = zeros(length(states)) # output
        for i ∈ eachindex(S) # foreach episode
            G = 0. # reset G foreach eps
            # compute the return G backwards:
            for t ∈ reverse(eachindex(S[i]))
                G = γ*G + R[i][t] # discount G_{t+1}
                # for first exit MC, check if S[t] is in S[1:t-1]:
                if S[i][t] ∉ S[i][1:t-1]
                    n[S[i][t]] += 1 # update counter
                    Vs[S[i][t]] += G # update return of S_t
                end
            end
        end
        return Vs ./ n # average V(s) := V(s)/n(s)
    end
\end{jllisting}
For EVMC, the code is as follows:
\begin{jllisting}
    function evmc(states, S, R, γ)
        n = zeros(Int, length(states)) # n_ts[i] := counter of visiting state i
        Vs = zeros(length(states)) # output
        for i ∈ eachindex(S) # foreach episode
            G = 0. # reset G foreach eps
            # compute the return G backwards:
            for t ∈ reverse(eachindex(S[i]))
                G = γ*G + R[i][t] # discount G_{t+1}
                n[S[i][t]] += 1 # update counter
                Vs[S[i][t]] += G # update return of S_t
            end
        end
        return Vs ./ n # average V(s) := V(s)/n(s)
    end
\end{jllisting}
Given trajectories $[A,A,B,T], [A,B,A,B,T]$ and $[A,B,T]$ sampled from policy $\pi$ and $\gamma = .5$ encoded as
\begin{jllisting}
    S = [[1,1,2],[1,2,1,2], [1,2]] # 1a:  [A, A, B, T], [A, B, A, B, T], [A, B, T]
    A = [[1,2,1],[2,2,2,1], [2,1]] # actions dont matter for computation
    R = Vector{Vector{Float64}}([[1,0,2], [0,1,0,2], [0,2]])
\end{jllisting}
we can call the FVMC by:
\begin{jllisting}
    Vs = fvmc(["A", "B"], S, R, γ)  # fvmc caller
\end{jllisting}
here we obtain the estimated value functions:
\begin{jllisting}
    Vs = [1.0833333333333333, 1.8333333333333333]
\end{jllisting}
i.e., 
\begin{equation*}
    V^{\pi}(A) = 1.083, ~~ V^{\pi}(B) = 1.833.
\end{equation*}
Similarly for FVMC, we call
\begin{jllisting}
    Vs = fvmc(["A", "B"], S, R, γ)  # fvmc caller
\end{jllisting}
and we get
\begin{equation*}
    V^{\pi}(A) = 1.05, ~~ V^{\pi}(B) = 1.875.
\end{equation*}

\subsubsection*{b)}
TD0:
\begin{jllisting}
    function TD0(states, S, R, γ, α)
        V = zeros(length(states)) # output
        for t ∈ eachindex(S)
            if t == length(S) # last index
                V[S[t]] += (α*(R[t] - V[S[t]]))  # terminal valuefun = 0
            else
                V[S[t]] += (α*(R[t] + γ*V[S[t+1]] - V[S[t]]))
            end
            println([t, S[t], R[t], V[S[t]]])
        end
        return V
    end
\end{jllisting}
TD2:
\begin{jllisting}
    function TD2(states, S, R, γ, α)
        V = zeros(length(states)) # output
        for t ∈ eachindex(S)
            if t == length(S) # last index
                V[S[t]] += (α*(R[t] - V[S[t]]))  # terminal approx value of the state = 0
            elseif t == length(S)-1 # 2nd last index, only the V_s_t+2 is 0 (which is the terminal), but terminal has reward: r_t+2 > 0
                V[S[t]] += (α*(R[t] + γ*R[t+1] - V[S[t]]))
            else
                V[S[t]] += (α*(R[t] + γ*R[t+1] + γ^2*V[S[t+2]] - V[S[t]])) # observed reward + observed reward of next state + approx reward of 2 next state 
            end
            println([t, S[t], R[t], V[S[t]]])
        end
        return V
    end
\end{jllisting}

SARSA:
\begin{jllisting}
    function SARSA(states, acts, S, A, R, γ, α)
        Q = zeros(length(states), length(acts)) # is a matrix instead of a vector like V
        for t ∈ eachindex(S)
            if t == length(S) # similar story to TD
                Q[S[t], A[t]] += ( α*(R[t] - Q[S[t], A[t]]) )
            else
                Q[S[t], A[t]] += ( α*(R[t] + γ*Q[S[t+1], A[t+1]] - Q[S[t], A[t]]) ) # predetermined policy returns A[t]
            end
            println([Int(t), Int(S[t]), Int(A[t]), R[t], Q[S[t], A[t]]])
        end
        return Q
    end
\end{jllisting}

\subsection*{2. Control with $\epsilon$-greedy policy}
\subsubsection*{a)}
Given an $\epsilon$-soft policy algorithm, show:
\begin{equation}
	Q^{\pi_t}(s,\pi_{t+1}(s)) \geq V^{\pi_t}(s).
\end{equation}
To show this, we expand the LHS, $\forall s$:
\begin{equation}
	\begin{split}
		Q^{\pi_t}(s,\pi_{t+1}(s)) &= \mathbb{E}_{a\sim\pi'(s,a)}[Q^{\pi_{t}}(s,a)] \\
		&= \sum_{a \in \mathcal{A}} P^{\pi_{t+1}}(a|s)Q^{\pi_{t}}(s,a) \\
		&= P(a^*|s)Q^{\pi_{t}}(s,a^*) + \sum_{a \neq a^*}P(a|s)Q^{\pi_{t}}(s,a) \\
		&=\Big(1-\epsilon+\frac{\epsilon}{|\mathcal{A}|}\Big)Q^{\pi_{t}}(s,a^*) + \Big(\epsilon - \frac{\epsilon}{|\mathcal{A}|}\Big)Q^{\pi_{t}}(s,a) \\
		&\geq Q^{\pi_{t}}(s,a) \geq V^{\pi_t}(s).
	\end{split}
\end{equation}

\subsubsection*{b)}
An MDP $\tilde{\mathcal{M}}$ is defined by
\begin{equation}
	\begin{split}
		\tilde{P}(s'|s,a) &= (1-\epsilon)P(s'|s,a) + \frac{\epsilon}{|\mathcal{A}|}\sum_{a' \in \mathcal{A}}P(s'|s,a'),\\
		\tilde{r}(s,a) &= (1-\epsilon)r(s,a) +  \frac{\epsilon}{|\mathcal{A}|}\sum_{a' \in \mathcal{A}}r(s,a'),
	\end{split}	
\end{equation}
from here, by using Bellman optimality equations, we can define analogously 
\begin{equation}
	\begin{split}
		\tilde{V}^*(s) &= \max_{a \in \mathcal{A}} \Big[\tilde{r}(s,a) + \gamma \sum_{s'\in S} \tilde{P}(s'|s,a)\tilde{V}^*(s')\Big] \\
		&= \max_{a \in \mathcal{A}} \Big[(1-\epsilon)r(s,a) +  \frac{\epsilon}{|\mathcal{A}|}\sum_{a' \in \mathcal{A}}r(s,a') + \gamma \sum_{s'\in S} \Big((1-\epsilon)P(s'|s,a) + \frac{\epsilon}{|\mathcal{A}|}\sum_{a' \in \mathcal{A}}P(s'|s,a')\Big)\Big] \\
		&= \frac{\epsilon}{|\mathcal{A}|}\sum_{a' \in \mathcal{A}}r(s,a') + \frac{\gamma \epsilon}{|\mathcal{A}|}\sum_{s'\in S, a' \in \mathcal{A}}P(s'|s,a') + \max_{a \in \mathcal{A}} \Big[(1-\epsilon)r(s,a) + \sum_{s' \in S} (\gamma-\gamma\epsilon)P(s'|s,a')\Big].
	\end{split}
\end{equation}

\subsubsection*{c)} \at{TBD}
\subsection*{3. Alternative proof of policy gradient}
\at{TBD}
\subsection*{4. Policy optimization under different parameterization}
Given the same Markov decision process as in \textbf{problem 1 of Assignment 1}:
\subsubsection*{a)}
a policy class parameterized by two parameters:
\begin{equation*}
    \pi(\text{stay}|A) = \theta_1, ~~ \pi(\text{switch}|B) = \theta_2, ~~ \theta_1, \theta_2 \in [0,1], ~~ \mathbf{\boldsymbol\theta} = (\theta_1, \theta_2),
\end{equation*}
has the value function $V^{\pi(\boldsymbol\theta)}(A)$
\begin{equation*}
    \begin{split}
        V^{\pi(\boldsymbol\theta)}(A) &= \sum_a \pi(a|A)Q^{\pi(\boldsymbol{\theta})}(A,a)    \\
        &= \theta_1 Q^{\pi(\boldsymbol{\theta})}(A,\text{stay}) + (1-\theta_1)Q^{\pi(\boldsymbol{\theta})}(A,\text{switch})\\
        &= \theta_1r(A,\text{switch}) + \gamma\theta_1\sum_{s'}P(s'|A,\text{stay})V^{\pi(\boldsymbol\theta)}(s')\\ & + (1-\theta_1)r(A,\text{stay}) + (1-\theta_1)\gamma\sum_{s'}P(s'|A,\text{switch})V^{\pi(\boldsymbol\theta)}(s') \\
        &= \theta_1 + \gamma\theta_1V^{\pi(\boldsymbol\theta)}(A) + (1 - \theta_1)\gamma V^{\pi(\boldsymbol\theta)}(B),
    \end{split}
\end{equation*}
similarly for $V^{\pi(\boldsymbol\theta)}(B)$
\begin{equation*}
    V^{\pi(\boldsymbol\theta)}(B) = \theta_2 + \gamma\theta_2V^{\pi(\boldsymbol\theta)}(A) + (1-\theta_2)\gamma V^{\pi(\boldsymbol\theta)}(B).
\end{equation*}
Then we simplify $V^{\pi(\boldsymbol\theta)}(A)$ using $V^{\pi(\boldsymbol\theta)}(B)$, vice versa for $V^{\pi(\boldsymbol\theta)}(B)$
\begin{equation*}
    \begin{split}
        V^{\pi(\boldsymbol\theta)}(A) &= \theta_1 + \gamma\theta_1V^{\pi(\boldsymbol\theta)}(A) + \frac{(\gamma - \gamma\theta_1)(\theta_2 + \gamma\theta_2V^{\pi(\boldsymbol\theta)}(A))}{1-\gamma+\gamma\theta_2}     \\
        &= \frac{\theta_1 - \gamma(\theta_1 - \theta_2)}{(\gamma-1)(\gamma(\theta_1 - \theta_2)-1)}, \\
        V^{\pi(\boldsymbol\theta)}(B) &= \frac{\theta_2}{(\gamma-1)(\gamma(\theta_1 - \theta_2)-1)}.
    \end{split}
\end{equation*}
The concavity can be tested by checking whether each value function has negative-semidefinite Hessian matrix.
\subsubsection*{b)}
Given a policy class:
\begin{equation}
    \pi(\text{stay}|A) = \pi(\text{switch}|B) = \theta,
\end{equation}
hence
\begin{equation*}
    \theta_1 = \theta_2 = \theta,
\end{equation*}
setting both $\theta_1 = \theta$ and $\theta_2 = \theta$, we have
\begin{equation*}
    V^{\pi(\boldsymbol\theta)}(A) = V^{\pi(\boldsymbol\theta)}(B) = \frac{\theta}{1-\gamma}.
\end{equation*}
It is clear that both value functions are linear, which imply the concavity.
\subsubsection*{c)}
Similarly to \textbf{2.b)}, given
\begin{equation*}
    \pi(\text{stay}|A) = \pi(\text{switch}|B) = \Big(\theta - \frac{1}{4}\Big)^2, ~~ \theta \in [0, 5/4],
\end{equation*}
then
\begin{equation*}
    V^{\pi(\boldsymbol\theta)}(A) = V^{\pi(\boldsymbol\theta)}(B) = \frac{\Big(\theta - \frac{1}{4}\Big)^2}{1-\gamma}.
\end{equation*}

\newpage
\begin{appendices}
    Julia code for \textbf{problem 1 of assignment 2}:
    \jlinputlisting{rl.jl}
\end{appendices}
\end{document}